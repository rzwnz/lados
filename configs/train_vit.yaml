# Training configuration for ViT-B/16

# Data
data:
  root: "data/processed"
  train_manifest: "manifests/train.csv"
  val_manifest: "manifests/val.csv"
  test_manifest: "manifests/test.csv"
  img_size: 224
  num_workers: 4

# Model
model:
  backbone: "vit_b_16"
  num_classes: 6
  pretrained: true
  freeze_backbone: false

# Training
training:
  epochs: 50
  batch_size: 8  # ViT uses more memory
  batch_size_tuner:
    enabled: true
    max_batch_size: 16
    min_batch_size: 2
  gradient_accumulation_steps: 4  # Higher to compensate for smaller batch
  mixed_precision: true
  
  optimizer: "AdamW"
  lr: 3e-4
  lr_finetune: 1e-5
  weight_decay: 1e-2
  
  scheduler: "cosine"
  warmup_epochs: 5
  
  early_stopping:
    enabled: true
    patience: 10
    metric: "val_macro_f1"
    mode: "max"
    min_delta: 0.001

# Output
output:
  run_dir: "runs"
  save_best_by: "val_macro_f1"
  export_formats: ["torchscript", "onnx"]

# Device
device:
  use_gpu: true
  cuda_device: 0

# Logging
logging:
  log_interval: 10
  save_interval: 1

# Small sample mode
small_sample:
  enabled: false
  max_samples: 200

